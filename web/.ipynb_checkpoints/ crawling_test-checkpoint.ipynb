{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "source = requests.get(\"https://www.naver.com/\").text\n",
    "soup = BeautifulSoup(source, \"html.parser\")\n",
    "hotKeys = soup.select(\"span.keyword\")\n",
    "\n",
    "index = 0\n",
    "for key in hotKeys:\n",
    "    index +=1\n",
    "    print(str(index) + \", \" + key.text)\n",
    "    if index >=20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "with open(\"ex01.html\") as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 코드와 동일\n",
    "fp = open(\"ex01.html\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>Web Crawling Example</title>\n",
       "<meta charset=\"utf-8\"/>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p>a</p>\n",
       "<p>b</p>\n",
       "<p>c</p>\n",
       "</div>\n",
       "<div class=\"ex_class\">\n",
       "<p>1</p>\n",
       "<p>2</p>\n",
       "<p>3</p>\n",
       "</div>\n",
       "<div id=\"ex_id\">\n",
       "<p>X</p>\n",
       "<p>Y</p>\n",
       "<p>Z</p>\n",
       "</div>\n",
       "<h1>This is a heading.</h1>\n",
       "<p>this is paragragh.</p>\n",
       "<p>this is another paragragh.</p>\n",
       "<a class=\"a\" href=\"https://www.naver.com\">\n",
       "            Naver\n",
       "        </a>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "with urllib.request.urlopen(\"https://www.naver.com\") as response:\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "<p>a</p>\n",
      "<p>b</p>\n",
      "<p>c</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fp = open(\"ex01.html\")\n",
    "soup = BeautifulSoup(fp, 'html.parser')\n",
    "\n",
    "first_div = soup.find(\"div\")\n",
    "print(first_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div>\n",
      "<p>a</p>\n",
      "<p>b</p>\n",
      "<p>c</p>\n",
      "</div>, <div class=\"ex_class\">\n",
      "<p>1</p>\n",
      "<p>2</p>\n",
      "<p>3</p>\n",
      "</div>, <div id=\"ex_id\">\n",
      "<p>X</p>\n",
      "<p>Y</p>\n",
      "<p>Z</p>\n",
      "</div>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<div id=\"ex_id\">\n",
       "<p>X</p>\n",
       "<p>Y</p>\n",
       "<p>Z</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_divs = soup.find_all(\"div\")\n",
    "print(all_divs)\n",
    "soup.find_all(\"div\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"ex_id\">\n",
      "<p>X</p>\n",
      "<p>Y</p>\n",
      "<p>Z</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "ex_id_divs = soup.find('div',{'id':'ex_id'})\n",
    "print(ex_id_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"ex_class\">\n",
      "<p>1</p>\n",
      "<p>2</p>\n",
      "<p>3</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "result = soup.find('div', class_ = 'ex_class')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"ex_class\">\n",
      "<p>1</p>\n",
      "<p>2</p>\n",
      "<p>3</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "class_res = soup.find('div', {'class':'ex_class'})\n",
    "print(class_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>X</p>, <p>Y</p>, <p>Z</p>]\n"
     ]
    }
   ],
   "source": [
    "ex_id_divs= soup.find(\"div\", {\"id\":\"ex_id\"})\n",
    "all_ps_in_ex_id_divs = ex_id_divs.find_all(\"p\")\n",
    "print(all_ps_in_ex_id_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "<p>a</p>\n",
      "<p>b</p>\n",
      "<p>c</p>\n",
      "</div>\n",
      "\n",
      "<div class=\"ex_class\">\n",
      "<p>1</p>\n",
      "<p>2</p>\n",
      "<p>3</p>\n",
      "</div>\n",
      "\n",
      "<div id=\"ex_id\">\n",
      "<p>X</p>\n",
      "<p>Y</p>\n",
      "<p>Z</p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_divs = soup.find_all('div')\n",
    "for each_div in all_divs:\n",
    "    print(each_div)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>\n",
       "<p>a</p>\n",
       "<p>b</p>\n",
       "<p>c</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_divs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na\\nb\\nc\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_divs[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "b\n",
      "c\n",
      "\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "\n",
      "\n",
      "X\n",
      "Y\n",
      "Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for divs_item in all_divs:\n",
    "    print(divs_item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='ex_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLT None\n",
      "Fried Bologna None\n",
      "Woodland Mushroom None\n",
      "Roast Beef None\n",
      "PB&L None\n",
      "Belgian Chicken Curry Salad None\n",
      "Lobster Roll None\n",
      "Smoked Salmon Salad None\n",
      "Atomica Cemitas None\n",
      "Grilled Laughing Bird Shrimp and Fried Po’ Boy None\n",
      "Ham and Raclette Panino None\n",
      "Breaded Steak None\n",
      "The Hawkeye None\n",
      "Chicken Dip None\n",
      "Wild Boar Sloppy Joe None\n",
      "Meatball Sub None\n",
      "Corned Beef None\n",
      "Turkey Club None\n",
      "Falafel None\n",
      "Crab Cake None\n",
      "Chicken Schnitzel None\n",
      "Shawarma None\n",
      "Toasted Pimiento Cheese None\n",
      "Vegetarian Panino None\n",
      "Cali Chèvre None\n",
      "Pastrami None\n",
      "The Fredo None\n",
      "Smoked Ham None\n",
      "Jibarito None\n",
      "Shaved Prime Rib None\n",
      "Serrano Ham and Manchego Cheese None\n",
      "Tuna Salad None\n",
      "Paramount Reuben None\n",
      "The Istanbul None\n",
      "B.A.D. None\n",
      "Duck Confit and Mozzarella None\n",
      "Croque Monsieur None\n",
      "Green Garbanzo None\n",
      "The Hen House None\n",
      "Tuscan Chicken None\n",
      "The Marty  None\n",
      "Whitefish None\n",
      "Oat Bread, Pecan Butter, and Fruit Jam None\n",
      "Cauliflower Melt None\n",
      "Cubana None\n",
      "Kufta None\n",
      "Debbie’s Egg Salad None\n",
      "Beef Curry None\n",
      "Le Végétarien None\n",
      "The Gatsby None\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "with urllib.request.urlopen(\"https://goo.gl/wAtv1s\") as response:\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "re = []\n",
    "re2 = []\n",
    "cl = soup.find_all('div', {'class':'sammyListing'})\n",
    "\n",
    "for result in cl:\n",
    "    re = result.find(\"b\").get_text()\n",
    "    print(re)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$10. 2109 W. Chicago Ave., 773-772-0406, theoldoaktap.com\n"
     ]
    }
   ],
   "source": [
    "with urllib.request.urlopen(\"https://www.chicagomag.com/Chicago-Magazine/November-2012/Best-Sandwiches-in-Chicago-Old-Oak-Tap-BLT/\") as response:\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "cl2 = soup.find('p', {'class':'addy'})\n",
    "result2 = cl2.find(\"em\")\n",
    "print(result2.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
